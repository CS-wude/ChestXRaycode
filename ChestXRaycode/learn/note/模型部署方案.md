# 模型部署方案详解

## 📋 目录
- [Ollama与图像模型的关系](#ollama与图像模型的关系)
- [直接部署方案](#直接部署方案)
- [多模态AI系统](#多模态ai系统)
- [与Ollama集成方案](#与ollama集成方案)
- [其他部署选项](#其他部署选项)
- [实施步骤](#实施步骤)

---

## Ollama与图像模型的关系

### 🤔 重要澄清

**Ollama的主要用途**：
- Ollama是用于运行**大语言模型（LLMs）**的工具
- 支持的模型：Llama、Mistral、CodeLlama、Vicuna等
- 主要处理：文本输入→文本输出

**你的模型类型**：
- 胸部X光片分类器是**图像分类模型**
- 基于PyTorch + ResNet架构
- 处理：图像输入→分类结果

### 💡 可能的集成场景

虽然不能直接将图像分类模型导入Ollama，但可以创建集成系统：

```
用户上传X光片 → 图像分类模型预测 → 结果传给LLM → 生成医学报告
```

---

## 直接部署方案

### 1. **REST API部署** (推荐)

创建一个Web API服务来部署你的模型：

```python
# 文件名: ChestXRay/learn/api_server.py
from flask import Flask, request, jsonify
from deploy_simple import ChestXRayPredictor
import base64
import io
from PIL import Image

app = Flask(__name__)
predictor = ChestXRayPredictor('checkpoints/best_model.pth')

@app.route('/predict', methods=['POST'])
def predict():
    # 接收图片并预测
    image_data = request.json['image']
    # 处理base64图片
    image = Image.open(io.BytesIO(base64.b64decode(image_data)))
    result = predictor.predict_single_image(image)
    return jsonify(result)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### 2. **FastAPI部署** (高性能)

```python
# 文件名: ChestXRay/learn/fastapi_server.py
from fastapi import FastAPI, File, UploadFile
from deploy_simple import ChestXRayPredictor
import uvicorn

app = FastAPI(title="胸部X光片分类API")
predictor = ChestXRayPredictor('checkpoints/best_model.pth')

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    contents = await file.read()
    result = predictor.predict_single_image(io.BytesIO(contents))
    return result

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 3. **Docker容器化**

```dockerfile
# Dockerfile
FROM pytorch/pytorch:latest

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["python", "fastapi_server.py"]
```

---

## 多模态AI系统

### 🎯 目标：图像分析 + 智能报告生成

创建一个结合图像分类和语言模型的系统：

```
X光片 → 图像分类 → 结构化结果 → LLM → 详细医学报告
```

### 架构设计

```python
# 系统架构示意
class MultimodalMedicalAI:
    def __init__(self):
        self.image_classifier = ChestXRayPredictor('checkpoints/best_model.pth')
        self.llm_client = OllamaClient()  # 连接Ollama
    
    def analyze_xray(self, image_path):
        # 1. 图像分类
        classification_result = self.image_classifier.predict_single_image(image_path)
        
        # 2. 构建LLM提示
        prompt = self.build_medical_prompt(classification_result)
        
        # 3. LLM生成报告
        medical_report = self.llm_client.generate(prompt)
        
        return {
            'classification': classification_result,
            'medical_report': medical_report
        }
```

---

## 与Ollama集成方案

### 方案1：API接口集成

**步骤**：
1. 部署图像分类模型为API服务
2. 使用Ollama运行医学领域的LLM
3. 创建集成服务连接两者

```python
# 集成服务示例
import requests
import json

class XRayOllamaIntegration:
    def __init__(self):
        self.xray_api = "http://localhost:8000"  # 图像分类API
        self.ollama_api = "http://localhost:11434"  # Ollama API
    
    def analyze_and_report(self, image_path):
        # 1. 图像分类
        with open(image_path, 'rb') as f:
            files = {'file': f}
            classification = requests.post(f"{self.xray_api}/predict", files=files).json()
        
        # 2. 构建医学提示
        prompt = f"""
        作为一名放射科医生，请根据以下AI分析结果生成详细的医学报告：
        
        分类结果：{classification['predicted_class']}
        置信度：{classification['confidence']:.2%}
        概率分布：{classification['probabilities']}
        
        请提供：
        1. 影像学描述
        2. 诊断意见  
        3. 建议的后续检查
        4. 注意事项
        """
        
        # 3. LLM生成报告
        ollama_data = {
            "model": "llama2",  # 或其他医学模型
            "prompt": prompt,
            "stream": False
        }
        
        response = requests.post(f"{self.ollama_api}/api/generate", json=ollama_data)
        medical_report = response.json()['response']
        
        return {
            'classification_result': classification,
            'medical_report': medical_report
        }
```

### 方案2：本地集成服务

```python
# 文件名: ChestXRay/learn/multimodal_service.py
import ollama
from deploy_simple import ChestXRayPredictor
import json

class MedicalMultimodalAI:
    def __init__(self, model_path, llm_model="llama2"):
        # 初始化图像分类器
        self.predictor = ChestXRayPredictor(model_path)
        self.llm_model = llm_model
        
        # 检查Ollama连接
        try:
            ollama.list()
            print("✅ Ollama连接成功")
        except Exception as e:
            print(f"❌ Ollama连接失败: {e}")
    
    def create_medical_prompt(self, classification_result):
        """创建医学报告生成提示"""
        if 'error' in classification_result:
            return f"图像分析失败：{classification_result['error']}"
        
        prompt = f"""
你是一名经验丰富的放射科医生。请根据AI辅助诊断系统的分析结果，生成一份专业的胸部X光片诊断报告。

AI分析结果：
- 预测类别：{classification_result['predicted_class']}
- 置信度：{classification_result['confidence']:.1%}
- 各类别概率：
  * 正常 (NORMAL): {classification_result['probabilities']['NORMAL']:.1%}
  * 肺炎 (PNEUMONIA): {classification_result['probabilities']['PNEUMONIA']:.1%}

请按以下格式生成报告：

## 胸部X光片诊断报告

**患者信息**: [AI辅助诊断]
**检查日期**: {classification_result['prediction_time'][:10]}

**影像学描述**:
[基于AI分析结果描述影像特征]

**诊断意见**:
[基于分类结果给出诊断意见]

**建议**:
[根据诊断结果提供医学建议]

**备注**:
- 本报告基于AI辅助分析，仅供临床参考
- 最终诊断需结合临床表现和其他检查
- 如有疑问请咨询专科医生

请确保报告专业、准确、符合医学标准。
"""
        return prompt
    
    def analyze_xray_with_report(self, image_path):
        """完整的X光片分析和报告生成"""
        print(f"🔍 正在分析X光片: {image_path}")
        
        # 1. 图像分类
        classification = self.predictor.predict_single_image(image_path)
        
        if 'error' in classification:
            return {
                'error': classification['error'],
                'image_path': image_path
            }
        
        print(f"📊 分类完成: {classification['predicted_class']} (置信度: {classification['confidence']:.1%})")
        
        # 2. 生成医学报告
        prompt = self.create_medical_prompt(classification)
        
        try:
            print("📝 正在生成医学报告...")
            response = ollama.generate(
                model=self.llm_model,
                prompt=prompt
            )
            medical_report = response['response']
            print("✅ 报告生成完成")
        except Exception as e:
            medical_report = f"报告生成失败: {e}"
            print(f"❌ 报告生成失败: {e}")
        
        return {
            'image_analysis': classification,
            'medical_report': medical_report,
            'combined_assessment': {
                'risk_level': self._assess_risk_level(classification),
                'confidence_interpretation': self._interpret_confidence(classification['confidence']),
                'recommendations': self._generate_recommendations(classification)
            }
        }
    
    def _assess_risk_level(self, classification):
        """评估风险等级"""
        if classification['predicted_class'] == 'PNEUMONIA':
            if classification['confidence'] >= 0.9:
                return "高风险 - 强烈建议立即就医"
            elif classification['confidence'] >= 0.7:
                return "中风险 - 建议尽快就医"
            else:
                return "低风险 - 建议医疗复查"
        else:
            if classification['confidence'] >= 0.9:
                return "正常 - 无明显异常"
            else:
                return "不确定 - 建议专业评估"
    
    def _interpret_confidence(self, confidence):
        """解释置信度"""
        if confidence >= 0.95:
            return "AI模型对此诊断非常确信"
        elif confidence >= 0.85:
            return "AI模型对此诊断比较确信"
        elif confidence >= 0.7:
            return "AI模型对此诊断有一定把握"
        else:
            return "AI模型对此诊断不够确定，建议人工复查"
    
    def _generate_recommendations(self, classification):
        """生成建议"""
        recommendations = [
            "本结果仅供参考，不能替代专业医生诊断",
            "如有症状或担忧，请及时就医"
        ]
        
        if classification['predicted_class'] == 'PNEUMONIA':
            if classification['confidence'] >= 0.8:
                recommendations.extend([
                    "建议进行血常规检查",
                    "可考虑痰培养检查",
                    "必要时进行CT检查"
                ])
            else:
                recommendations.extend([
                    "建议密切观察症状变化",
                    "如出现发热、咳嗽加重等症状请及时就医"
                ])
        
        return recommendations

def main():
    """演示多模态医学AI系统"""
    import argparse
    
    parser = argparse.ArgumentParser(description='多模态医学AI诊断系统')
    parser.add_argument('--model', type=str, default='checkpoints/best_model.pth',
                       help='图像分类模型路径')
    parser.add_argument('--image', type=str, required=True,
                       help='X光片图像路径')
    parser.add_argument('--llm', type=str, default='llama2',
                       help='Ollama LLM模型名称')
    parser.add_argument('--output', type=str,
                       help='报告输出文件路径')
    
    args = parser.parse_args()
    
    # 创建多模态AI系统
    try:
        ai_system = MedicalMultimodalAI(args.model, args.llm)
    except Exception as e:
        print(f"❌ 系统初始化失败: {e}")
        return
    
    # 分析图像并生成报告
    result = ai_system.analyze_xray_with_report(args.image)
    
    if 'error' in result:
        print(f"❌ 分析失败: {result['error']}")
        return
    
    # 打印结果
    print("\n" + "="*80)
    print("🏥 多模态医学AI诊断结果")
    print("="*80)
    
    print(f"\n📊 图像分析结果:")
    print(f"   预测类别: {result['image_analysis']['predicted_class']}")
    print(f"   置信度: {result['image_analysis']['confidence']:.1%}")
    print(f"   风险评估: {result['combined_assessment']['risk_level']}")
    
    print(f"\n📝 AI生成的医学报告:")
    print(result['medical_report'])
    
    print(f"\n💡 系统建议:")
    for rec in result['combined_assessment']['recommendations']:
        print(f"   • {rec}")
    
    # 保存报告
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\n📄 完整报告已保存至: {args.output}")

if __name__ == "__main__":
    main()
```

---

## 其他部署选项

### 1. **ONNX格式转换**

将PyTorch模型转换为ONNX格式，提高兼容性：

```python
# 转换脚本
import torch
import torch.onnx
from model import create_model

def convert_to_onnx(model_path, onnx_path):
    # 加载模型
    model = create_model(num_classes=2, model_name='resnet50')
    checkpoint = torch.load(model_path, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # 创建示例输入
    dummy_input = torch.randn(1, 3, 224, 224)
    
    # 转换为ONNX
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output']
    )
```

### 2. **TorchScript序列化**

```python
# 序列化为TorchScript
def convert_to_torchscript(model_path, script_path):
    model = create_model(num_classes=2, model_name='resnet50')
    checkpoint = torch.load(model_path, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # 转换为TorchScript
    traced_model = torch.jit.trace(model, torch.randn(1, 3, 224, 224))
    traced_model.save(script_path)
```

### 3. **云端部署**

```yaml
# Kubernetes部署配置
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chest-xray-classifier
spec:
  replicas: 3
  selector:
    matchLabels:
      app: chest-xray-classifier
  template:
    metadata:
      labels:
        app: chest-xray-classifier
    spec:
      containers:
      - name: classifier
        image: your-registry/chest-xray-classifier:latest
        ports:
        - containerPort: 8000
```

---

## 实施步骤

### 🚀 快速开始：创建多模态AI系统

#### 步骤1：安装Ollama

```bash
# 安装Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 启动Ollama服务
ollama serve

# 下载医学相关模型（在新终端）
ollama pull llama2  # 或者其他适合的模型
ollama pull codellama  # 用于代码生成
ollama pull mistral   # 更快的模型
```

#### 步骤2：安装Python依赖

```bash
cd ChestXRay/learn
pip install ollama flask fastapi uvicorn
```

#### 步骤3：创建多模态服务

我已经为你创建了完整的代码，你可以直接使用：

```bash
# 测试多模态AI系统
python multimodal_service.py --image ../../data/ChestXRay/test/PNEUMONIA/person1_virus_11.jpeg --output report.json
```

#### 步骤4：部署为Web服务

```python
# 创建Web界面（可选）
from flask import Flask, render_template, request, jsonify
from multimodal_service import MedicalMultimodalAI

app = Flask(__name__)
ai_system = MedicalMultimodalAI('checkpoints/best_model.pth')

@app.route('/')
def index():
    return render_template('upload.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    file = request.files['xray']
    result = ai_system.analyze_xray_with_report(file)
    return jsonify(result)
```

### 🎯 选择最适合的方案

**根据你的需求选择**：

1. **只需要图像分类** → 使用 `deploy_simple.py`
2. **需要智能医学报告** → 使用多模态AI系统
3. **需要Web服务** → 部署FastAPI或Flask
4. **需要生产环境** → Docker + Kubernetes

### 💡 下一步建议

1. **先测试基础功能**：
   ```bash
   python deploy_simple.py --image path/to/xray.jpg
   ```

2. **然后尝试多模态系统**：
   ```bash
   # 确保Ollama运行
   ollama serve
   
   # 在新终端运行
   python multimodal_service.py --image path/to/xray.jpg
   ```

3. **根据效果决定进一步开发方向**

---

## 总结

虽然不能直接将图像分类模型导入Ollama，但可以创建更强大的多模态AI系统：

- **图像分析** + **智能报告生成**
- **专业医学建议** + **风险评估**
- **可扩展的部署方案**

这样的系统比单纯的模型更有实用价值！ 